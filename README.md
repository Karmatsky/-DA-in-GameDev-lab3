# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил:
- Кармацкий Савелий Андреевич
- РИ210942
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | # | 60 |
| Задание 2 | # | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Разработка системы машинного обучения

## Задание 1 

## Задание 2 
Decision Requester - запрашивает решение через регулярные промежутки времени

Behavior Parameters - определяет принятие объектом решений, в него указывается какой тип поведения будет использоваться: уже обученная модель или удалённый процесс обучения

```
behaviors:
RollerBall: # указываем id агента
trainer_type: ppo # режим обучения (Proximal Policy Optimization)
hyperparameters:
batch_size: 10 # количество опытов на каждой итерации
buffer_size: 100 # количество опыта, которое нужно набрать перед обновлением модели
learning_rate: 3.0e-4 # начальная скорость обучения
beta: 5.0e-4 # сила регуляции энтропии, увеличивает случайность действий
epsilon: 0.2 # порог расхождений между старой и новой политиками при обновлении
lambd: 0.99 # параметр регуляции, насколько сильно агент полагается на текущий value estimate
num_epoch: 3 # количество проходов через буфер опыта, при выполнении оптимизации
learning_rate_schedule: linear # определяет как скорость обучения изменяется с течением времени, linear линейно уменьшает скорость
network_settings:
normalize: false # отключаем нормализацию входных данных
hidden_units: 128 # количество нейронов в скрытых слоях сети
num_layers: 2 # количество скрытых слоёв в сети
reward_signals:
extrinsic:
gamma: 0.99 # коэффициент скидки для будущих вознаграждений
strength: 1.0 # коэффициент на который умножается вознаграждение
max_steps: 500000 # общее количество шагов, которые должны быть выполнены в среде до завершения обучения
time_horizon: 64 # сколько опыта нужно собрать для каждого агента, прежде чем добавлять его в буфер
summary_freq: 10000 # количество опыта, который необходимо собрать перед созданием и отображением статистики
```

## Задание 3 

## Выводы


| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

